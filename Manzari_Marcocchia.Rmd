---
title: "Homework3"
author: "Andrea Marcocchia and Matteo Manzari"
date: "28/01/2018"
output: 

  html_document:
       
       toc: true
       toc_float: true
       number_sections: false
       code_folding: show
       theme: sandstone
       highlight: tango
       fig_width: 15
       fig_height: 13
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```

#<span style="color:red">Import packages</span>

```{r import-package}
require(tseries,quietly = T)
library(igraph)
require(RCurl)
library(pcaPP)
library(doParallel)
library(knitr)
library(dplyr)
```


With this two commands we register the parameters in order to work in parallel.

```{r register_parallel}
cl <- makeCluster(2)
registerDoParallel(cl)
```


#<span style="color:red">Obtain data</span>

In this homework we want to study the **dependency among some standard measures of stock relative performance**. To this end, we may collect the daily closing prices for **100 stocks**, selected within those consistently in the **S&P500 index** between 01/01/2003 and 01/01/2008, before the onset of the "financial crisis".

The tickers of the 100 stocks are stored in our GitHub (click [here](https://raw.githubusercontent.com/andremarco/statHW3/master/ticker.csv)). In order to obtain a sample we wrote an R code, reported below. Not to waste time, we stored data on GitHub, so that it is possible to avoid sampling every time.
The 100 stocks are selected so that we have 10 from each **GICS** (*Global Industry Classification Standard*):

* Industrials
* Health Care
* Information Technology
* Consumer Discretionary
* Utilities
* Financials
* Materials
* Consumer Staples	
* Real Estate	
* Energy

Unfortunately we cannot make sampling from *Telecommunications Services* GICS, because the selected sample size (10 for each GICS) is bigger than that GICS size (only 4 stocks).


```{r load-ticker}
# Read data from GitHub repository
ticker <- read.csv(text=getURL("https://raw.githubusercontent.com/andremarco/statHW3/master/ticker.csv"), header=T)

# Add rownames
row.names(ticker) <- ticker$X

# Delete X column, where the rownames are stored
ticker$X <- NULL

# Transform ticker in a dataframe
ticker <- as.matrix(ticker)
```

The tickers sample is obtained with the following code. **This code is not going to be run.** The code is reported just to show how it works.


```{r eval=FALSE}

# Read data from the database with S&P 500 stocks stored on GitHub
data <-read.csv(text=getURL("https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"), header=T)
data <- as.matrix(data)

ris <- NULL
conta = 0

# Use a loop to analyse all the tickers of S&P500
# Store in ris only the tickers without errors

for (k in 1:length(data[,1]))
{
  aut = 0
  
  # Use tryCatch function to find any type of error
  x <- tryCatch({
    aux <- suppressWarnings(get.hist.quote(instrument = data[k],start = "2003-01-01",end = "2008-01-01",quote = "Close",provider = "yahoo",drop=T)) 
    aut = 1
    },error = function(e) aut=0
  )
  
  # If aut==1 it means no error
  if (aut==1) {
    
    chec <- sum(is.na(aux))
    
    # Check if there are NA values in the zoo time series
    # If there aren't NA, store that ticker in ris
    if (chec==0) 
      {
      
      # Store the stock ticker in aux_bis
      aux_bis = data[k] 
      
      # Store the stock GICS in aux_tris
      aux_tris = unname(data[k,3])
    
      # Add a new row to ris dataframe
      # In the new line there is data on the stock ticker and the stock GICS
      ris = rbind(ris, data.frame(aux_bis,aux_tris))
      conta=conta+1
    }

    }
}

# Change the names of the dataframe columns
names(ris) <- c("symbol","GICS")


# Create a set of the GICS
GICS <-levels(ris$GICS)


# Create a loop to analyse all the GICS (11)
# For each GICS, create a new variable in the RStudio Global Enviroment to store all the tickers of that particular GICS
for (i in 1:length(GICS))
{
  ll <- ris[ris$GICS==GICS[i],]
  
  # Creatre new variable
  assign(paste("GICS",i,sep=""), ll)    

}


# Make a sample of size 10 from each GICS variable 
# Store the 10 selected elements from each GICS in a dataframe
final_sticker <- NULL
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS1$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS2$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS3$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS4$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS5$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS6$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS7$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS8$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS9$symbol,10))))
final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS10$symbol,10))))


# the 11th GICS, Telecomunications Services, has less than 10 elements
# so it is not included in the sample


# final_sticker <- rbind(final_sticker, levels(droplevels(sample(GICS11$symbol,10))))

# This is our final sample, stored on GitHub
final_sticker
```

**Coming back to the running code**, we have a dataframe with all the tickers (100) needed to work.

The dataframe is the following one:

```{r final_sticker}
kable(ticker)
```


We obtain information about these stocks from Yahoo Financial provider. We ask only for the **closing quote**.
In particular, for each stock, we are interested in a trasformation of the closing quote. If we call $x_{t,j}$ the closing price of the $j^{th}$ stock at the *t* time, we calculate:
\[
log(\dfrac{x_{t,j}}{x_{t-1,j}})
\]

and create a new dataframe, called *data_def*, where these values are saved.


```{r}

mom <- NULL
for (i in 1:length(ticker[,1]))
{
  for (k in 1:length(ticker[,1]))
  {
    # Obtain data from Yahoo Finance
    aux <-  suppressWarnings(get.hist.quote(instrument = ticker[i,k],start = "2003-01-01",end = "2008-01-01",quote ="Close",provider = "yahoo",drop=T,quiet=T))
  aux2 <- as.matrix(aux)
  
  
  aux3 <- NULL
  # Evaluate the requested quantity
  for (j in 1:length(aux2)-1)
  {
    aux3[j] <- log(unname(aux2[j+1])/unname(aux2[j]))
  }
  mom <- cbind(mom,aux3)
  }
}

# transform data_def in a dataframe
data_def=as.data.frame(mom)

# change the dataframe names
# column names are the stocks name
names(data_def) <- t(ticker)

# row names are the days, excluded the first one, observed from Yahoo Finance
row.names(data_def) <-row.names(aux2)[2:length(aux2)]


```


#<span style="color:red">A bit of theory: marginal correlation graph</span>

In a marginal correlation graph we put an edge between two nodes ($V_k$ and $V_j$) if 
\[
\left|\rho(j,k)\right|\geq\epsilon
\]

The parameter $\rho(j,k)$ has to satisfy this very important **property**, that for sake of simplicity we call *independence property* in our work: 
\[
X\perp Y \Rightarrow \rho(j,k)=0
\]

Besides this property, we would like $\rho$ to have several other **properties**:

1. easy to compute;
2. robust to outliers;
3. there must be some way to calculate a confidence interval for the parameter.


It is possible to use many different indexes to build a marginal correlation graph. We decides to use **Kendall** and **Spearman** correlation indexes, because of their strength and their acceptable time complexity.
We analyzed also other indexes before selecting Kendall and Spearman ones. For example, the **Pearson** linear correlation index was a good candidate because it is very fast to compute and it satisfies the independece property, but it is not robust to outliers. Another possible candidate was the **distance correlation** index. This index verify the strong version of the independece property, that is to say:

\[
X\perp Y \Leftrightarrow \rho(j,k)=0
\]

This index is robust to outliers as well, but it is extremely hard to compute, because its time complexity is $O(n^2)$.

Considering all the elements reported above, we will go on using Kendall and Spearman indexes.


Now, using a bootstrap procedure, we proceed building two **simultaneus confidence intervals** $C_{j,k}(\alpha)$, one for Kendall and one for Spearman index. We will use this confidence interval to decide if it is the case to put a link between nodes $j$ and $k$, according to this formula:

\[
[-\epsilon,+\epsilon]\cap C_{j,k}(\alpha) = \emptyset
\]

where $\epsilon \in [0,1]$ .

**A simultaneaus confidence interval could be obtained by using a non-parametric bootstrap procedure**, and we will apply this method in the next section. After the boostrap we will be able to build an **empirical comulative density function** for the requested quantity. With this ECDF we can also evaluate the quantile, necessary to build the confidence interval. Our final confidence interval will be expressed in the following form:
\[
C_{j,k}(\alpha)=\bigg[\hat{R}[j,k]-\dfrac{t_{\alpha}}{\sqrt{n}},\hat{R}[j,k]+\dfrac{t_{\alpha}}{\sqrt{n}}\bigg]
\]

Where $t_{\alpha}$ is the ECDF quantile at the level $\alpha$ and $\hat{R}[j,k]$ is the correlation matrix evaluated in the stocks sample matrix obtained before the bootstrap procedure.


Remembering that $\left|\rho(j,k)\right|\geq\epsilon$, the quantity __$\epsilon$__ represents the correlation lower bound that we want to study.
To understand when we add an edge between two nodes, we can analyse the following plot, based on this formula: $[-\epsilon,+\epsilon]\cap C_{j,k}(\alpha) = \emptyset$


```{r}
# Define a function to draw curly braces
CurlyBraces <- function(x, y, range, pos = 1, direction = 1 ) {
  
  a=c(1,2,3,48,50)    # set flexion point for spline
  b=c(0,.2,.28,.7,.8) # set depth for spline flexion point
  
  curve = spline(a, b, n = 50, method = "natural")$y / 2 
  
  curve = c(curve,rev(curve))
  
  a_sequence = rep(x,100)
  b_sequence = seq(y-range/2,y+range/2,length=100)  
  
  # direction
  if(direction==1)
    a_sequence = a_sequence+curve
  if(direction==2)
    a_sequence = a_sequence-curve
  
  # pos
  if(pos==1)
    lines(a_sequence,b_sequence) # vertical
  if(pos==2)
    lines(b_sequence,a_sequence) # horizontal
  
}

# Plot the 4 cases

par(mfrow=c(2,2))

# case 1

x <- c(1,2,3,4,5)
y <- c(3,3,3,3,3)
epsmeno <- c(2,3,4)
epsilpiu <- c(5,5,5)

plot(x,y,type='l',ylim=c(0,10),xlim=c(0,6),col='chocolate3',lwd=4)
points(epsmeno,epsilpiu,type='l',col='blue',lwd=4)
text(x=1,y=2.2,labels = 'L.C',col='chocolate3',cex=2.3)
text(x=5,y=2.2,labels = 'U.C',col='chocolate3',cex=2.3)
text(x=2,y=6,labels = expression(-epsilon),col='blue',cex=2.3)
text(x=4,y=6,labels = expression(+epsilon),col='blue',cex=2.3)

lines(c(1,1,1), c(2.8,3,3.2) ,col="chocolate3",lwd=3)
lines(c(5,5,5), c(2.8,3,3.2) ,col="chocolate3",lwd=3)
lines(c(2,2,2), c(4.8,5,5.2) ,col="blue",lwd=3)
lines(c(4,4,4), c(4.8,5,5.2) ,col="blue",lwd=3)
lines(c(2,2,2,2,2,2),c(0,1,2,3,4,5),lty=4,lwd=2)
lines(c(4,4,4,4,4,4),c(0,1,2,3,4,5),lty=4,lwd=2)

CurlyBraces(3,3,2,2,2)

text(x=3,y=2,labels='Intersection',cex=1.9)
text(x=3,y=9,labels=expression(bold("No edge")),col='red',cex=4)

# case 2
eps2 <- c(1,2,3,4,5)
epsi2 <- c(3,3,3,3,3)
x_2 <- c(2,3,4)
y_2 <- c(5,5,5)

plot(x_2,y_2,type='l',ylim=c(0,10),xlim=c(0,6),col='chocolate3',lwd=4,xlab='x',ylab='y')
points(eps2,epsi2,type='l',col='blue',lwd=4)
text(x=1,y=2.2,labels = expression(-epsilon),col='blue',cex=2.3)
text(x=5,y=2.2,labels = expression(+epsilon),col='blue',cex=2.3)
text(x=2,y=6,labels = 'L.C',col='chocolate3',cex=2.3)
text(x=4,y=6,labels = 'U.C',col='chocolate3',cex=2.3)

lines(c(1,1,1), c(2.8,3,3.2) ,col="blue",lwd=3)
lines(c(5,5,5), c(2.8,3,3.2) ,col="blue",lwd=3)
lines(c(2,2,2), c(4.8,5,5.2) ,col="chocolate3",lwd=3)
lines(c(4,4,4), c(4.8,5,5.2) ,col="chocolate3",lwd=3)
lines(c(2,2,2,2,2,2),c(0,1,2,3,4,5),lty=4,lwd=2)
lines(c(4,4,4,4,4,4),c(0,1,2,3,4,5),lty=4,lwd=2)

CurlyBraces(3,3,2,2,2)

text(x=3,y=2,labels='Intersection',cex=1.9)
text(x=3,y=9,labels=expression(bold("No edge")),col='red',cex=4)

# case 3

eps3 <- c(1,2,3)
epsi3 <- c(3,3,3)
x_3 <- c(2,3,4)
y_3 <- c(5,5,5)
plot(x_3,y_3,type='l',ylim=c(0,10),xlim=c(0,6),col='chocolate3',lwd=4,xlab='x',ylab='y')
points(eps3,epsi3,type='l',col='blue',lwd=4)

text(x=1,y=2.2,labels = expression(-epsilon),col='blue',cex=2.3)
text(x=3,y=2.2,labels = expression(+epsilon),col='blue',cex=2.3)
text(x=2,y=6,labels = 'L.C',col='chocolate3',cex=2.3)
text(x=4,y=6,labels = 'U.C',col='chocolate3',cex=2.3)

lines(c(1,1,1), c(2.8,3,3.2) ,col="blue",lwd=3)
lines(c(3,3,3), c(2.8,3,3.2) ,col="blue",lwd=3)
lines(c(2,2,2), c(4.8,5,5.2) ,col="chocolate3",lwd=3)
lines(c(4,4,4), c(4.8,5,5.2) ,col="chocolate3",lwd=3)

lines(c(2,2,2,2,2,2),c(0,1,2,3,4,5),lty=4,lwd=2)
lines(c(3,3,3,3,3,3),c(0,1,2,3,4,5),lty=4,lwd=2)

CurlyBraces(3,2.5,1,2,1)

text(x=2.5,y=4,labels='Intersection',cex=1.3)
text(x=3,y=9,labels=expression(bold("No edge")),col='red',cex=4)

# Case 4
eps4 <- c(1,2,3)
epsi4 <- c(3,3,3)
x_4 <- c(4,5,6)
y_4 <- c(5,5,5)
plot(x_4,y_4,type='l',ylim=c(0,10),xlim=c(0,7),col='chocolate3',lwd=4,xlab='x',ylab='y')
points(eps4,epsi4,type='l',col='blue',lwd=4)

text(x=1,y=2.2,labels = expression(-epsilon),col='blue',cex=2.3)
text(x=3,y=2.2,labels = expression(+epsilon),col='blue',cex=2.3)
text(x=4,y=6,labels = 'L.C',col='chocolate3',cex=2.3)
text(x=6,y=6,labels = 'U.C',col='chocolate3',cex=2.3)

text(x=3.5,y=9,labels=expression(bold("Add edge")),col='green',cex=4)

par(mfrow=c(1,1))
```


#<span style="color:red">Correlation and bootstrap</span>
## Kendall


Kendall's $\tau$ is a measure of the **rank correlation between two measured quantities**. Consider $(X,Y)$ and $(X',Y')$, a couple of random indipendent and identically distributed variables, generated by the joint distribution $F_{x,y}(x,y)$. We say that $(X,Y)$ and $(X',Y')$ are **concordant** if $(X-X')(Y-Y')\geq 0$ and discordant otherwise.

Kendall's $\tau$ is defined as:
\[
\tau(X,Y)= P(concordant)- P(discordant)= P((X-X')(Y-Y')\geq 0)-P((X-X')(Y-Y')<0)
\]

This index returns a value of 0 if $X$ and $Y$ are independent and **it's robust to outliers**. 
**By using the package PCApp this index is easy to compute**. At the end the Kendall's $\tau$ respects all the requested properties and it's a perfect parameter to build our marginal correlation graph.

The index calculation running time by using the PCApp package is $O\big(nlog(n)\big)$, instead of $O(n^2)$, used in the current R implementation. 


Analyzing the "cor" function to calculate the rank correlation Kendall's $\tau$, we verify that it isn't efficient to evaluate this index with large samples, because each of the $N$ pairs, $(X_i, Y_i)$, is compared with all the other pairs, $(X_j, Y_j)$, having a running time of order $O(n^2)$. The process of $\tau$ calculation is closely related to that of ordering a list of numbers in internal storage, i.e. performing an internal sort. Anyway internal sorting algorithms with running time of order $O\big(nlog(n)\big)$ are known.
Considering this relation between $\tau$ calculation and an internal sort, it exists a method with running time of order $O\big(nlog(n)\big)$ also for to evaluate the Kendall correlation index. This is the idea used in the PCApp package, that we use to evaluate the Kendall correlation index in a faster way.



```{r kendall-correlation}
# cor.kf is the function to evaluate the Kendall index in the PCApp package
R_kendall <- cor.fk(data_def)
```


At this point we have a correlation matrix, *R_kendall*, but we need something more. In fact we need to work with more data, so that we can evalute a **confidence interval for the correlation index**.
Bootstrap method is a resampling from the observed data. We will do it 1000 times. In every bootstrap repetition we will obtain a sample of the same original length, where there could be repetitions, so that the new dataset will be a bit different from the original one, and consequently also the correlation matrix will be a bit different.

In every bootstrap repetition we will store in a vector (*delta*) the following quantity:
\[
\Delta_b = \sqrt{n}*\underset{j,k}{\max}{\left|\hat{R_{b}^{*}}[j,k]-\hat{R} [j,k]\right|}
\]

where:

+ $\hat{R_{b}^{*}}[j,k]$ is the correlation matrix of the *b* bootstrap sample 
+ $\hat{R} [j,k]$ is the original correlation matrix

The *deltas* vector is evaluated by using a **parallel for-loop**. As the bootstrap resamplings are independent among them, we can run the 1000 repetitions in parallel, using less time. To write loops in parallel programming, we use the __*doParallel*__ package and *foreach* loop-statement.


```{r bootstrap-1}
B=1000
n=length(data_def[,1])

# Start the bootstrap loop
delta <- foreach (i = 1:B,.packages='pcaPP') %dopar%
{
  # Sampling with replacemente from the observed day
  idx <- sample(1:n,replace=T)
  data_boot <- NULL

  # Start the loop used to analyze all the element in the matrix
  for (j in 1:length(data_def))
  {
    xx <- data_def[,j][idx]
    data_boot <- cbind(data_boot,xx)
  }
  boot_R_star= cor.fk(data_boot)

  # Evaluate the requested quantity of interest
  sexy<- abs(R_kendall - boot_R_star)
  maxy<- max(sexy)
  maxy*((n)^(1/2))
}

delta <- as.numeric(delta)
```

Brush up about delta distribution:

```{r plot}
hist(delta,prob=T, col='orchid',border='black')
curve(dnorm(x,mean=mean(delta),sd=sd(delta)),add=T, col = "red",lwd=4)
```



Now we proceed by calculating the **empirical comulative density function** (or ECDF) from the *delta* vector. 
The ECDF is defined as follows:
\[
\hat{F}_{(n)}(t) = \dfrac{1}{B}\sum_{b=1}^{B}{\mathbb{I}(\Delta_b\leq t)}
\]

and this is the ECDF plot:
 
 
```{r ecdf-1}
# Create the ECDF from delta
cdf<-ecdf(delta)

plot(ecdf(delta),col = 'red', main = "Plot of ECDF")

```

Now we create the confidence intervals, working with ECDF of delta. For this first example, we use a typical value of $\alpha=0.05$.


```{r CI_Kendall}
# Create the lower bound of our confidence interval
L_c_kendall <- R_kendall - (unname(quantile(cdf, probs=0.95))) / ((n)^(1/2))

# Create the upper bound of our confidence interval
U_c_kendall <- R_kendall + (unname(quantile(cdf, probs=0.95))) / ((n)^(1/2))
```

Define an **epsilon** to build our marginal correlation graph.
```{r}
epsilon <- 0.4
```


Build the **graph**:
```{r graph-1}
# Create an empty graph
graph <- make_empty_graph(directed=F)

# Define a list of color
# The length of this list has to be the same of the number of GICS
# Every nodes belonging to different GICS will have a different color in the graph
col2= c("red","yellow","orchid","cyan","green","blue","orange","black","grey","brown")

# Add only nodes to the graph
for (i in 1:length(ticker[,1]))
{
  for (j in 1:length(ticker[,1]))
  {
    # The color of each node changes with the index of first loop (that is between 1:10)
    graph <- graph+ vertex(color= col2[i])
  }
}

# Use another loop to add edges
for (i in 1:length(ticker))
{
for (j in 1:length(ticker))
{
  # use this if statement to avoid the self edges 
  if (i!=j)
  {
  if ((-epsilon>U_c_kendall[i,j]) | (epsilon<L_c_kendall[i,j]))
  {
    # Add edges between nodes i and j
    graph <- graph + edge(i, j)
  }
  }
}
}

# Plot the obtained graph
plot.igraph(graph,vertex.size=4,vertex.label=NA)
legend(x='topleft',legend=row.names(ticker),col=col2,pch=19,cex=1.4,bty='n',pt.cex=2.2)
```

## Spearman

Now we apply the same procedures by using another correlation measure: **Spearman**, denoted with $r_s$.
The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of the two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of $+1$ or $-1$ occurs when each of the variables is a perfect monotone function of the other.
The Spearman correlation coefficient could be defined as the Pearson correlation coefficient between the ranked variables.
Writing it in formula, we obtain:
\[
r_s = \dfrac{cov(rg_x,rg_y)}{\sigma_{rg_x}\sigma_{rg_y}}
\]

Where $rg_x$  is the variable $X$ converted to rank, and $\sigma_{rg_y}$ is the standard deviation of the rank variable.



Create the correlation matrix for the true, observed, value using Spearman correlation index:

```{r spearman}
R_spearman <- cor(data_def,method = "spearman")
```


Apply the bootstrap resampling procedure, to obtain the *delta* (now called *delta_bis*) vector:

```{r bootstrap-2}
B=1000
n=length(data_def[,1])



# Start the bootstrap loop
delta_bis <- foreach (i = 1:B)  %dopar%
{
  # Sampling with replacemente from the observed day
  idx <- sample(1:n,replace=T)
  data_boot <- NULL
  
  # Start the loop used to analyze all the element in the matrix
  for (j in 1:length(data_def))
  {
    xx <- data_def[,j][idx]
    data_boot <- cbind(data_boot,xx)
  }
  boot_R_star= cor(data_boot , method= 'spearman')

  # Evaluate the requested quantity of interest
  sexy<- abs(R_spearman - boot_R_star)
  maxy<- max(sexy)
  maxy*((n)^(1/2))
}

```

```{r}
delta_bis <- as.numeric(delta_bis)
```


Study the behaviour of *delta_bis* distribution, by plotting its histogram and the related Normal distribution, using *delta_bis* mean and standard error as parameters:

```{r plot-2}
hist(delta_bis,prob=T,col='orchid',border='black')
curve(dnorm(x,mean=mean(delta_bis),sd=sd(delta_bis)),add=T, col = "red",lwd=4)
```

Create the empirical comulative density function:

```{r ecdf-2}
# Create the ECDF from delta_bis
cdf_bis<-ecdf(delta_bis)
```

Build the confidence interval using Spearman correlation and fixing $\alpha$ at its typical value $0.05$:

```{r IC}
# Create the lower bound of our confidence interval
L_c_spearman <- R_spearman - (unname(quantile(cdf_bis, probs=0.95))) / ((n)^(1/2))

# Create the upper bound of our confidence interval
U_c_spearman <- R_spearman + (unname(quantile(cdf_bis, probs=0.95))) / ((n)^(1/2))
```

And now we create the graph, using the just calculated interval:

```{r graph-2}
epsilon_bis <- 0.4

# Create an empty graph
graph_bis <- make_empty_graph(directed=F)

# Define the same list of color used in the previous case (pearson correlartion graph)
col1= c("red","yellow","orchid","cyan","green","blue","orange","black","grey","brown")

# Add only nodes to the graph
for (i in 1:length(ticker[,1]))
{
  for (j in 1:length(ticker[,1]))
  {
    graph_bis <- graph_bis + vertex(color= col1[i])
  }
}

# Add edges to the graph
for (i in 1:length(ticker))
{
  for (j in 1:length(ticker))
  {
    if (i!=j)
    {
      if ((-epsilon_bis>U_c_spearman[i,j]) | (epsilon_bis<L_c_spearman[i,j]))
      {
        graph_bis <- graph_bis + edge(i, j)
      }
    }
  }
}


# Plot the graph
plot.igraph(graph_bis,vertex.size=4,vertex.label=NA)
legend(x='topleft',legend=row.names(ticker),col=col1,pch=19,cex=1.4,bty='n',pt.cex=2.2)
```


#<span style="color:red">Results and considerations</span>

Now we can check the changements in the graph if the $\alpha$ value and $\epsilon$ value are different. **To make these plots we'll use the Spearman correlation index.** Anyway the results of changing the parameters are more or less the same if we use Kendall or Spearman index.

Starting from a value of $\alpha=0.05$ and $\epsilon=0$, the graph appears extremely dense, as it's plotted below.

```{r}
epsilon_dense <- 0.0

# Create an empty graph
graph_dense <- make_empty_graph(directed=F)

# Define the same list of color used in the previous case (pearson correlartion graph)
col1= c("red","yellow","orchid","cyan","green","blue","orange","black","grey","brown")

# Add only nodes to the graph
for (i in 1:length(ticker[,1]))
{
  for (j in 1:length(ticker[,1]))
  {
    graph_dense <- graph_dense + vertex(color= col1[i])
  }
}

# Add edges to the graph
for (i in 1:length(ticker))
{
  for (j in 1:length(ticker))
  {
    if (i!=j)
    {
      if ((-epsilon_dense>U_c_spearman[i,j]) | (epsilon_dense<L_c_spearman[i,j]))
      {
        graph_dense <- graph_dense + edge(i, j)
      }
    }
  }
}


# Plot the graph
plot.igraph(graph_dense,vertex.size=4,vertex.label=NA)
legend(x='topleft',legend=row.names(ticker),col=col1,pch=19,cex=1.4,bty='n',pt.cex=2.2)
```


By taking smaller values of alpha, the number of edges in the graph will decrease, in fact the confidence interval will be larger and larger when alpha decrease.

With the **dynamic graph below**, it is possible to visualize the changement that occurs when we modify both $\alpha$ and $\epsilon$.
In the plot we'll see the graph and the number of edges. We also evaluate the percentage of edges compared to the maximum possible number of edges in the graph, that is $100*999=9900$:


```{r dynamic-plot ,fig.height = 25, fig.width = 15, fig.align = "center"}

ui <- fluidPage(
  titlePanel("Interactive graph plot"),
  sidebarLayout(
    sidebarPanel(
      sliderInput("alpha", "alpha", 0, 0.05, 0.05,step=0.001),
      sliderInput("epsilon", "epsilon", 0, 0.7, 0.3,step=0.1)
    ),
    mainPanel(
      # Create different panel in output
      tabsetPanel(
        tabPanel("Plot",plotOutput("alpha")),
        tabPanel("Statistiscs",textOutput("var1"),textOutput("var2"))
      )
      
    )
  )
)


server <- function(input, output) {
  output$alpha <- renderPlot({
    L_c_din <- R_spearman - (unname(quantile(cdf_bis, probs=(1-input$alpha)))) / ((n)^(1/2))
    
    U_c_din <- R_spearman + (unname(quantile(cdf_bis, probs=(1-input$alpha)))) / ((n)^(1/2))
    graph_dinamic <- make_empty_graph(directed=F)
    
    
    col2= c("red","yellow","orchid","cyan","green","blue","orange","black","grey","brown")
    contatore=0
    # Add only nodes to the graph
    for (i in 1:length(ticker[,1]))
    {
      for (j in 1:length(ticker[,1]))
      {
        # The color of each node changes with the index of first loop (that is between 1:10)
        graph_dinamic <- graph_dinamic+ vertex(color= col2[i])
      }
    }
    
    # Use another loop to add edges
    for (i in 1:length(ticker))
    {
      for (j in 1:length(ticker))
      {
        # use this if statement to avoid the self edges 
        if (i!=j)
        {
          if ((-input$epsilon>U_c_din[i,j]) | (input$epsilon<L_c_din[i,j]))
          {
            # Add edges between nodes i and j
            graph_dinamic <- graph_dinamic + edge(i, j)
            contatore=contatore+1
          }
        }
      }
    }
    output$var1 <- renderText(paste('Number of edges in graph: ', contatore))
    
    output$var2 <- renderText(paste("Percentage of edge: ",round((contatore/9900)*100,2),"%"))
    # Plot the obtained graph
    plot.igraph(graph_dinamic,vertex.size=4,vertex.label=NA)
  })
  
}

# Set options for plot size
options=list(height=500,width=800)

# Create the shint app
shinyApp(ui = ui, server = server,options = options)


```


Analyzing the statistics about the graph behaviour, we can proof that, decreasing $\alpha$ and fixing $\epsilon$, we obtain graphs with a smaller number of edges.


We proceed in our work by fixing the statistical parameter $\alpha$ at the value of $0.05$, obtaining a **confidence interval at level 95%**.
From now, we will change only the "interestingness" parameter $\epsilon$, to analyse the behaviour of the GICS at different levels of correlation.



Define a vector of $\epsilon$:
```{r}
epsilon_vec=c(0.6,0.5,0.4,0.3)
```

Draw the four different graphs, using the $\epsilon$ values stored in *epsilon_vec*:

```{r}
# Create the lower bound of our confidence interval
alphaa<- 0.05
L_c_spearman <- R_spearman - (unname(quantile(cdf_bis, probs=1-alphaa))) / ((n)^(1/2))

# Create the upper bound of our confidence interval
U_c_spearman <- R_spearman + (unname(quantile(cdf_bis, probs=1-alphaa))) / ((n)^(1/2))
par(mfrow=c(1,2),oma=c(0,0,0,0))
for (eps in 1:length(epsilon_vec))
{

# Create an empty graph
graph_loop <- make_empty_graph(directed=F)

# Define the same list of color used in the previous case (pearson correlartion graph)
col1= c("red","yellow","orchid","cyan","green","blue","orange","black","grey","brown")

# Add only nodes to the graph
for (i in 1:length(ticker[,1]))
{
  for (j in 1:length(ticker[,1]))
  {
    graph_loop <- graph_loop + vertex(color= col1[i])
  }
}

# Add edges to the graph
for (i in 1:length(ticker))
{
  for (j in 1:length(ticker))
  {
    if (i!=j)
    {
      if ((-epsilon_vec[eps]>U_c_spearman[i,j]) | (epsilon_vec[eps]<L_c_spearman[i,j]))
      {
        graph_loop <- graph_loop + edge(i, j)
      }
    }
  }
}

# Plot the graph
stringa = paste('epsilon =',epsilon_vec[eps])
plot.igraph(graph_loop,vertex.size=4,vertex.label=NA,main=stringa)

}

par(mfrow=c(1,1))
```


We can note that if $\epsilon$ is equal to 0.6 the most linked sector is the grey one (Real Estate). It means that in this sector there is an high dependecy between companies, and the stocks behaviour is similar when there is a changement in the market.
When $\epsilon$ is equal to $0.5$ there are two new sub-graphs, formed by companies of Utilities GICS (green) and Energy (brown). If $\epsilon$ is further small we notice that various GICS are linked each others. When epsilon is 0.3 we obtain a very connected sub-graph, with most of the GICS inside. Green nodes, that represent the Utility GICS, are isolated, and linked only between them. It means that Utilities sector seems to be not correlated with the other GICS.


We can also compare the two graphs, obtained using Kendall and Spearman correlation indexes, to understand the differences, using the same $\epsilon$ and the same $\alpha$:

```{r}
par(mfrow=c(1,2),oma=c(0,0,0,0))
plot.igraph(graph_bis,vertex.size=4,vertex.label=NA,main='Spearman correlation')
plot.igraph(graph,vertex.size=4,vertex.label=NA, main='Kendall correlation')
```

We decide to use Kendall's $\tau$ and Spearman's $\rho$ because those measures have all the properties we need for our parameters. Both Spearman and Kendall measure monotonicity relationships.
The basic difference is that Spearman's $\rho$ is an attempt to extend $R^2$ (="variance explained") idea over nonlinear interactions, while Kendall's $\tau$ is rather intended to be a statistic test for nonlinear correlation.
We can notice that **Kendall correlation has a smaller asymptotic variance than the Spearman one**. Because of it, Kendall's $\tau$ confidence interval at level $1-alpha$ is smaller than the Spearman's confidence interval at the same level. We can see an evidence of it by analyzing this R code:

```{r check}
sum(((U_c_kendall-L_c_kendall)-(U_c_spearman-L_c_spearman))>0)
```

Thanks to this result we imagine to have more links with the Spearman correlation, but we should consider that **Spearman correlation index has, on average, bigger correlation between stocks than Kendall**. 
This is the Kendall correlation mean:
```{r}
mean(R_kendall)
```


and this is the Spearman correlation mean:

```{r}
mean(R_spearman)
```

We are looking at $\epsilon$ equal to 0.4, and we'll have more links in Spearman graph because its correlation, on average, is bigger than the Kendall one.




